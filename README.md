# Emotion and Action Prediction using Deep Learning

## Introduction

This project aims to classify emotions and predict actions based on facial expressions and enviromental analysis using deep convolutional neural networks (CNNs) and machine learning models. The system is trained on the **FER-2013** dataset for emotion detection and the **EMOTIC** and **MPII Human Pose** datasets for action prediction.

The model classifies a person's emotion into one of **seven categories** (angry, disgusted, fearful, happy, neutral, sad, and surprised) and predicts possible actions based on facial expressions and environmental context.

## Features

- **Emotion Detection**: Classifies facial expressions into seven emotions.
- **Action Prediction**: Uses mapped datasets to predict potential actions based on detected emotions.
- **Live Video Processing**: Real-time emotion and action recognition using webcam feed.
- **Deep Learning Model Optimization**: Improved accuracy through dataset enhancements and hyperparameter tuning.

## Dependencies

* Python 3
* [OpenCV](https://opencv.org/)
* [TensorFlow](https://www.tensorflow.org/)
* [NumPy](https://numpy.org/)
* [Matplolib](https://matplotlib.org)
* [Scipy](https://scipy.org)
* [Pandas](https://pandas.pydata.org)

To install all dependencies, run:
```bash
pip install -r requirements.txt
```

## Directory Structure
```bash
Emotify/
â”‚â”€â”€ .gitignore
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ imgs/
â”‚â”€â”€ src/
â”‚   â”‚â”€â”€ data/  # Contains datasets (ignored in .gitignore)
        â”‚â”€â”€ test/
        â”‚â”€â”€ train/
â”‚   â”‚â”€â”€ images/ # Contains image samples (ignored in .gitignore)
â”‚   â”‚â”€â”€ emotions.py  # Emotion detection script
â”‚   â”‚â”€â”€ action.py  # Action prediction script
â”‚   â”‚â”€â”€ action_mapping.py
â”‚   â”‚â”€â”€ actions.txt
â”‚   â”‚â”€â”€ dataset_prepare.py  # Data preprocessing
â”‚   â”‚â”€â”€ haarcascade_frontalface_default.xml  # Haar Cascade
â”‚   â”‚â”€â”€ load_mpii.py 
â”‚   â”‚â”€â”€ model.h5  # Pre-trained model weights
â”‚   â”‚â”€â”€ mpii_annotations.csv
â”‚   â”‚â”€â”€ mpii_human_pose_v1_u12_1.mat

```
## Basic Usage

The repository is currently compatible with `tensorflow-2.0` and makes use of the Keras API using the `tensorflow.keras` library.

* First, clone the repository and enter the folder

```bash
git clone https://github.com/miracneroid/Emotify.git
cd Emotion-detection
```

* Download the FER-2013 dataset inside the `src` folder.

* If you want to train this model, use:  

```bash
cd src
python emotions.py --mode train
```

* If you want to view the predictions without training again, you can download the pre-trained model from [here](https://drive.google.com/file/d/1Ohtj9Zamv71mSNrjO9o_iMQuoT_nFPlQ/view?usp=share_link) and then run:  

```bash
cd src
python emotions.py --mode display
```

* This implementation by default detects emotions on all faces in the webcam feed. With a simple 4-layer CNN, the test accuracy reached 63.2% in 50 epochs.

![Accuracy plot](imgs/accuracy.png)

## Data Preparation (optional)

* The [original FER2013 dataset in Kaggle](https://www.kaggle.com/deadskull7/fer2013) is available as a single csv file. I had converted into a dataset of images in the PNG format for training/testing.

* In case you are looking to experiment with new datasets, you may have to deal with data in the csv format. I have provided the code I wrote for data preprocessing in the `dataset_prepare.py` file which can be used for reference.

## Algorithm

* First, the **haar cascade** method is used to detect faces in each frame of the webcam feed.

* The region of image containing the face is resized to **48x48** and is passed as input to the CNN.

* The network outputs a list of **softmax scores** for the seven classes of emotions.

* The emotion with maximum score is displayed on the screen.

## References

* "Challenges in Representation Learning: A report on three machine learning contests." I Goodfellow, D Erhan, PL Carrier, A Courville, M Mirza, B
   Hamner, W Cukierski, Y Tang, DH Lee, Y Zhou, C Ramaiah, F Feng, R Li,  
   X Wang, D Athanasakis, J Shawe-Taylor, M Milakov, J Park, R Ionescu,
   M Popescu, C Grozea, J Bergstra, J Xie, L Romaszko, B Xu, Z Chuang, and
   Y. Bengio. arXiv 2013.

* FER2013 Dataset - Kaggle 
* MPII Human Pose Dataset

If you find any issues or need help, feel free to raise an issue or download the structured project from [Google Drive](https://drive.google.com/drive/folders/1W9JlTjq5G0kKuZV-Zmj0NSlZyzewpSCY?usp=share_link)

```bash
Let me know if you need any further modifications! ðŸš€
```
